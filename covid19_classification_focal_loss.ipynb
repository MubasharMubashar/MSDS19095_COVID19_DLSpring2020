{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "â€˜MSDS19095_05_part2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEfhhCTyawT3",
        "colab_type": "text"
      },
      "source": [
        "**Down Load DataSet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pI2D4c6MVu0U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gdown\n",
        "\n",
        "url = 'https://drive.google.com/a/itu.edu.pk/uc?id=1eytbwaLQBv12psV8I-aMkIli9N3bf8nO&export=download'\n",
        "\n",
        "gdown.download(url,'/content/drive/My Drive/Assignment 05_P2.zip', quiet=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxFzGzSOa8Ov",
        "colab_type": "text"
      },
      "source": [
        "**Unzip Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GC5628kLai1U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip  '/content/drive/My Drive/Assignment 05_P2.zip' -d  '/content/drive/My Drive'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ttp5Gvg-bD0F",
        "colab_type": "text"
      },
      "source": [
        "**Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e20wH2u8amQD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "import  numpy as np\n",
        "from tqdm import notebook\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYIVRIuUbHKz",
        "colab_type": "text"
      },
      "source": [
        "**Load Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LfkuwlTamd8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dir = '/content/drive/My Drive/A_05_Part_02_Dataset'\n",
        "#Define transforms for the training data and testing data\n",
        "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
        "                                       transforms.RandomResizedCrop(224),\n",
        "                                       transforms.RandomHorizontalFlip(),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                                            [0.229, 0.224, 0.225])])\n",
        "\n",
        "valid_transforms = transforms.Compose([transforms.Resize(256),\n",
        "                                      transforms.CenterCrop(224),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                                           [0.229, 0.224, 0.225])])\n",
        "\n",
        "#pass transform here-in\n",
        "train_data = datasets.ImageFolder(data_dir + '/Train', transform=train_transforms)\n",
        "valid_data = datasets.ImageFolder(data_dir + '/Validation', transform=valid_transforms)\n",
        "\n",
        "#data loaders\n",
        "trainloader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True,num_workers=2,pin_memory=True)\n",
        "validloader = torch.utils.data.DataLoader(valid_data, batch_size=16, shuffle=True,num_workers=2,pin_memory=True)\n",
        "\n",
        "print(\"Classes: \")\n",
        "class_names = train_data.classes\n",
        "print(class_names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7jNhAgpbuMp",
        "colab_type": "text"
      },
      "source": [
        "**One_Hot_encoding Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cq6eyaKgao6N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot_encode(X):\n",
        "  OneHot = []\n",
        "  for label in X:\n",
        "    if label == 0:\n",
        "      OneHot.append([1., 0., 1.])\n",
        "    if label == 1:\n",
        "      OneHot.append([0., 1., 0.])\n",
        "    if label == 2:\n",
        "      OneHot.append([0., 0., 1.])\n",
        "\n",
        "  return torch.FloatTensor(OneHot)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNoSdecGeBTR",
        "colab_type": "text"
      },
      "source": [
        "**Focal Loss Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5r7e-uiceaWD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2, reduce=True):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.gamma = gamma\n",
        "        self.reduce = reduce\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        BCEL = nn.BCEWithLogitsLoss()\n",
        "        BCE_loss = BCEL(inputs, targets)\n",
        "        pt = torch.exp(-BCE_loss)\n",
        "        F_loss = (1-pt)**self.gamma * BCE_loss\n",
        "        if self.reduce:\n",
        "            return torch.mean(F_loss)\n",
        "        else:\n",
        "            return F_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pG2szWqMLKP4",
        "colab_type": "text"
      },
      "source": [
        "**Training Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvNoSRIeLP6L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def training_fun(model,Epochs, optimizer, output_file_name,Criterion=nn.BCEWithLogitsLoss(),isFocal = False ):\n",
        "  #if you have gpu then you need to convert the network and data to cuda\n",
        "  #the easiest way is to first check for device and then convert network and data to device\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model.to(device)\n",
        "\n",
        "  model.train()\n",
        "  lossEpochs = []\n",
        "  accEpochs = []\n",
        "  lossValid = []\n",
        "  accValid = []\n",
        "  for epoch in range(Epochs):  # loop over the dataset multiple times\n",
        "      correct = 0\n",
        "      total = 0\n",
        "      running_loss = 0.0\n",
        "      pbar = tqdm(enumerate(trainloader))\n",
        "      for i, data in pbar:\n",
        "          # get the inputs\n",
        "          inputs, original_labels = data\n",
        "          labels = one_hot_encode(original_labels)\n",
        "          inputs, labels = inputs.to(device), labels.to(device)\n",
        "          # zero the parameter gradients\n",
        "          optimizer.zero_grad()\n",
        "          # In PyTorch, we need to set the gradients to zero before starting to do backpropragation \n",
        "          # because PyTorch accumulates the gradients on subsequent backward passes. \n",
        "          # This is convenient while training RNNs. \n",
        "          # So, the default action is to accumulate the gradients on every loss.backward() call\n",
        "\n",
        "          # forward + backward + optimize\n",
        "          outputs = model(inputs)               #----> forward pass\n",
        "          if isFocal==False:\n",
        "            loss = Criterion(outputs, labels)   #----> compute loss\n",
        "          else:\n",
        "            FL =  FocalLoss()\n",
        "            loss = FL.forward(outputs,labels)   #----> compute loss\n",
        "          loss.backward()                     #----> backward pass\n",
        "          optimizer.step()                    #----> weights update\n",
        "\n",
        "          sig = nn.Sigmoid()\n",
        "          val = sig(outputs)\n",
        "          val[val>=0.5]=1\n",
        "          val[val<0.5]=0\n",
        "          total += labels.size(0)\n",
        "\n",
        "          c_count=0\n",
        "          for j in range(0,labels.size(0)):\n",
        "            if(val[j]==labels[j]).sum().item()==3:\n",
        "              c_count+=1\n",
        "              correct+=1\n",
        "          # print statistics\n",
        "          running_loss += loss.item()\n",
        "          \n",
        "          pbar.set_description(\n",
        "              'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tCorrect: {}\\tTotal: {}'.format(\n",
        "                  epoch, i * len(inputs), len(trainloader.dataset),\n",
        "                  100. * i / len(trainloader),\n",
        "                  loss.data.item(),c_count,labels.size(0)))\n",
        "          \n",
        "\n",
        "      acc = 100 * correct / total\n",
        "      print('Training Accuracy of Epoch ' + str(epoch) + ': %d %%' % (acc))\n",
        "      lossEpochs.append(running_loss)\n",
        "      accEpochs.append(acc)\n",
        "\n",
        "      # # Validation Accuracy and Loss\n",
        "      correct = 0\n",
        "      total = 0\n",
        "      valid_loss = 0.0\n",
        "      confusionMatrix = np.zeros((2, 2));\n",
        "      with torch.no_grad():\n",
        "          for valid_data in validloader:\n",
        "              images, original_labels = data\n",
        "              labels =  one_hot_encode(original_labels)\n",
        "              images, labels = images.to(device), labels.to(device)\n",
        "              outputs = model(images)\n",
        "              if isFocal==False:\n",
        "                valid_losss = Criterion(outputs, labels)   #----> compute loss\n",
        "              else:\n",
        "                FL =  FocalLoss()\n",
        "                valid_losss = FL.forward(outputs,labels)   #----> compute loss\n",
        "              sig = nn.Sigmoid()\n",
        "              val = sig(outputs)\n",
        "              val[val>=0.5]=1.\n",
        "              val[val<0.5]=0.\n",
        "              # _, predicted = torch.max(outputs.data, 1)\n",
        "              total += labels.size(0)\n",
        "              c_count = 0\n",
        "              for j in range(0,labels.size(0)):\n",
        "                if(val[j]==labels[j]).sum()==3:\n",
        "                  c_count+=1\n",
        "                  correct+=1\n",
        "      acc = 100 * correct / total\n",
        "      print('Validation Accuracy of Epoch ' + str(epoch) + ': %d %%' % (acc))\n",
        "      lossValid.append(valid_loss)\n",
        "      accValid.append(acc)\n",
        "      torch.save(model.state_dict(), '/content/drive/My Drive/'+output_file_name)\n",
        "  plt.plot(lossEpochs)\n",
        "  plt.plot(lossValid)\n",
        "  plt.xlabel(\"No. of Epochs\")\n",
        "  plt.ylabel(\"Cross Entropy Loss\")\n",
        "  plt.show()\n",
        "\n",
        "  plt.plot(accEpochs)\n",
        "  plt.plot(accValid)\n",
        "  plt.xlabel(\"No. of Epochs\")\n",
        "  plt.ylabel(\"Accuracy\")\n",
        "  plt.show()\n",
        "  print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HH_5w7RjQymt",
        "colab_type": "text"
      },
      "source": [
        "**Testing Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TtIoXaiQ1im",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def testing_fun(model,dataset):\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model.to(device)\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  predicted_List=[]\n",
        "  labels_List=[]\n",
        "  predicted_cuda_List=[]\n",
        "  labels_cuda_List=[]\n",
        "  with torch.no_grad():\n",
        "      for data in dataset:\n",
        "          images, original_labels = data\n",
        "          labels = one_hot_encode(original_labels)\n",
        "          images, labels = images.to(device), labels.to(device)\n",
        "          labels_cuda_List.append(labels)\n",
        "          outputs = model(images)\n",
        "          sig = nn.Sigmoid()\n",
        "          val = sig(outputs)\n",
        "          val[val>=0.5]=1\n",
        "          val[val<0.5]=0\n",
        "          predicted_cuda_List.append(val)\n",
        "          total += labels.size(0)\n",
        "          c_count = 0\n",
        "          for j in range(0,labels.size(0)):\n",
        "            if(val[j]==labels[j]).sum()==3:\n",
        "              c_count+=1\n",
        "              correct+=1\n",
        "  for x in predicted_cuda_List:\n",
        "    for y in x:\n",
        "      predicted_List.append(y.cpu().numpy())\n",
        "  for x in labels_cuda_List:\n",
        "    for y in x:\n",
        "      labels_List.append(y.cpu().numpy())\n",
        "\n",
        "  labels_List = np.asarray(labels_List)\n",
        "  predicted_List = np.asarray(predicted_List)\n",
        "\n",
        "  F1_Score = f1_score(y_true = labels_List, y_pred= predicted_List, average='weighted')\n",
        "  conf_matrix = multilabel_confusion_matrix(labels_List,predicted_List)\n",
        "  acc = 100 * correct / total\n",
        "  print('Accuracy : %d %%' % (acc))\n",
        "  print('F1 Score : %d %%' % (F1_Score))\n",
        "  print('Covid19\\n ', conf_matrix[0])\n",
        "  print('Normal\\n  ',  conf_matrix[1])\n",
        "  print('Pneumonia\\n ',conf_matrix[2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNKjtBrcbPwW",
        "colab_type": "text"
      },
      "source": [
        "**Load VGGG16 Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBB8FqIbam7B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vgg16 = models.vgg16(pretrained=False)\n",
        "fl_neu_inp = vgg16.classifier[0].in_features\n",
        "features = list(vgg16.classifier)[:-7]\n",
        "features\n",
        "features.extend(\n",
        "    [\n",
        "     nn.Linear(fl_neu_inp,460),\n",
        "     nn.ReLU(inplace=True),\n",
        "     nn.Dropout(inplace=False),\n",
        "     nn.Linear(460, len(class_names))])\n",
        "vgg16.classifier = nn.Sequential(*features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2crj294bm-k",
        "colab_type": "text"
      },
      "source": [
        "**Set Hyper Parameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksXsLphPaowc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Epochs = 15\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.SGD(vgg16.parameters(), lr=0.001, momentum=0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qHoiFigb0al",
        "colab_type": "text"
      },
      "source": [
        "**Train Model With Out Focal Loss Using VGG16**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccxQYIShapBv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_fun(vgg16,Epochs,optimizer,\"new_vgg16_new\",Criterion=criterion)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f0PjRSzcaQz",
        "colab_type": "text"
      },
      "source": [
        "**Test On Validation Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FY8pSyIeam58",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testing_fun(vgg16,validloader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpmdA0yyc1gx",
        "colab_type": "text"
      },
      "source": [
        "**Load Resnet18 Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xe3ASio3c9We",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "resnet = models.resnet18(pretrained=False)\n",
        "resnet.fc = nn.Sequential(\n",
        "  nn.Linear(resnet.fc.in_features, 1050, bias=True),\n",
        "  nn.ReLU(inplace=True),\n",
        "  nn.Dropout(p=0.5, inplace=False),\n",
        "  nn.Linear(1050, len(class_names), bias=True)\n",
        ")\n",
        "# Freeze training for all layers\n",
        "for param in resnet.parameters():\n",
        "    param.requires_grad = True "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gg5WJxiydJ0d",
        "colab_type": "text"
      },
      "source": [
        "**Setting Hyper Parameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pmg4svGdN_U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Epochs = 15\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.SGD(resnet.parameters(), lr=0.001, momentum=0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IV9OK18XdQ9r",
        "colab_type": "text"
      },
      "source": [
        "**Traing On Resnet18 WithOut Focal Loss**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37k3mbQKdVcd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_fun(resnet,Epochs,optimizer,\"new_resnet_new\",Criterion=criterion)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPXsYv3Ld1JR",
        "colab_type": "text"
      },
      "source": [
        "**Testing on Validation Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1qw9ey-d5B0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testing_fun(resnet,validloader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ig9jTH4NeBVk",
        "colab_type": "text"
      },
      "source": [
        "**Changing Weights of VGG16 For Focal_Loss Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmnTDpMyeOXX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vgg16 = models.vgg16(pretrained=False)\n",
        "fl_neu_inp = vgg16.classifier[0].in_features\n",
        "features = list(vgg16.classifier)[:-7]\n",
        "features\n",
        "features.extend(\n",
        "    [\n",
        "     nn.Linear(fl_neu_inp,1000),\n",
        "     nn.ReLU(inplace=True),\n",
        "     nn.Dropout(inplace=False),\n",
        "     nn.Linear(1000, len(class_names))])\n",
        "vgg16.classifier = nn.Sequential(*features)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqiEwZWgehBP",
        "colab_type": "text"
      },
      "source": [
        "**Setting Hyper Parameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-3_wteLd5AP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Epochs = 15\n",
        "optimizer = optim.SGD(vgg16.parameters(), lr=0.001, momentum=0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8X8KmQ3pepcl",
        "colab_type": "text"
      },
      "source": [
        "**Training VGG16 With Focal Loss**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0FaPWquexzy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_fun(vgg16,Epochs,optimizer,\"resnet_new\",isFocal=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7TGPX1Be0wk",
        "colab_type": "text"
      },
      "source": [
        "**Testing On Validation Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8oGyy-be4VE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testing_fun(vgg16,validloader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5p1pblXzfNJu",
        "colab_type": "text"
      },
      "source": [
        "**Changing Weights of Resnet18 for tarining with Focal Loss**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pa7d77dWfTWo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "resnet.fc = nn.Sequential(\n",
        "  nn.Linear(resnet.fc.in_features, 256, bias=True),\n",
        "  nn.ReLU(inplace=True),\n",
        "  nn.Dropout(p=0.5, inplace=False),\n",
        "  nn.Linear(256, len(class_names), bias=True)\n",
        ")\n",
        "# Freeze training for all layers\n",
        "for param in resnet.parameters():\n",
        "    param.requires_grad = True "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZYHs4Vgfb3m",
        "colab_type": "text"
      },
      "source": [
        "**Setting Hyper Parameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpHBb8IWffvB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Epochs = 15\n",
        "optimizer = optim.SGD(resnet.parameters(), lr=0.001, momentum=0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJuOKX_Wfhr0",
        "colab_type": "text"
      },
      "source": [
        "**Tarining Resnet18 model With Focal Loss**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViPGavhcfmEn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_fun(resnet,Epochs,optimizer,\"resnet_new\",isFocal=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Vc238olfppC",
        "colab_type": "text"
      },
      "source": [
        "**Testing using Validation Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnJOmnOKfsl3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testing_fun(resnet,validloader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kB7n5vQzj-BP",
        "colab_type": "text"
      },
      "source": [
        "**Loading Test DataSet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnBbbkJVkBDi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "test_transforms = transforms.Compose([transforms.Resize(256),\n",
        "                                      transforms.CenterCrop(224),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                                           [0.229, 0.224, 0.225])])\n",
        "class ImageFolderWithPaths(datasets.ImageFolder):\n",
        "    # override the __getitem__ method. this is the method that dataloader calls\n",
        "    def __getitem__(self, index):\n",
        "        # this is what ImageFolder normally returns \n",
        "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
        "        # the image file path\n",
        "        path = self.imgs[index][0]\n",
        "        # make a new tuple that includes original and the path\n",
        "        tuple_with_path = (original_tuple + (path,))\n",
        "        return tuple_with_path\n",
        "\n",
        "test_data = ImageFolderWithPaths('/content/drive/My Drive/A_05_Part_02_Dataset/Test', transform=test_transforms)\n",
        "testloader = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zwmn8O-6kHdm",
        "colab_type": "text"
      },
      "source": [
        "**Predicting Test Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5a-QQvhRkLzq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "results = []\n",
        "with torch.no_grad():\n",
        "    predicted_labels = []\n",
        "    for data in testloader:\n",
        "        images, labels, paths = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = vgg16(images)\n",
        "\n",
        "        for predicted_output, path in zip(outputs.data, paths):\n",
        "          predicted = []\n",
        "          predicted.append(os.path.basename(path))\n",
        "  \n",
        "          predicted_output = torch.sigmoid(predicted_output)\n",
        "          thresholded_vector = (predicted_output >= 0.5).int()\n",
        "\n",
        "          thresholded_vector = thresholded_vector.tolist()\n",
        "          \n",
        "          # the order in which hidden results are present\n",
        "          thresholded_vector[1], thresholded_vector[2] = thresholded_vector[2], thresholded_vector[1]\n",
        "          predicted.extend(thresholded_vector)\n",
        "          \n",
        "          predicted_labels.append(predicted)\n",
        "    print(predicted_labels)\n",
        "    results = predicted_labels\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdTp8BV4kYyB",
        "colab_type": "text"
      },
      "source": [
        "**Saving Predicted Into csv File**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAq8K8u-kjn0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(predicted_labels)\n",
        "df.to_csv('MSDS19095_results.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}